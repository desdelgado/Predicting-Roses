{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Will you accept this model? Predicting love on the Bachelorette\n",
    "\n",
    "# Part 2 \n",
    "\n",
    "# Introduction\n",
    "\n",
    "Going forward, I would like to only look at the bachelorette seasons since that is currently airing and being played fantasy wise. As noted, we have exhausted the low hanging fruit from the 538 dataset and need to look elsewhere.\n",
    "\n",
    "When watching the most recent season, Hannah, the bachelorette, mentioned that one thing that makes one of her contestants, Jed, so attractive is that they come from the same southern background.  It's also been well documented that a good amount of people fall in people who are of similar backgrounds.  Thus, perhaps adding a categorical feature that tells us if the contestant is from the same cultural roots will help train our model.  To me, this question can be broken down into a few parts: same cultural region, same hometown, political leanings, and age.  I am sure there are other ways, but for now we'll focus on those four.\n",
    "\n",
    "Before we go any further let's import the libraries and datasets from the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "#Scrape Websites\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "elim_data = pd.read_csv('Bachelorette_Data/elim_data.csv')\n",
    "elim_data = elim_data.drop(['Unnamed: 0', 'index'], axis = 1)\n",
    "\n",
    "data_table = pd.read_csv('Bachelorette_Data/data_table.csv')\n",
    "data_table = data_table.drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape and Organize Contestant Information from Wikipedia\n",
    "\n",
    "Let's start with figuring out the cultural region.\n",
    "\n",
    "To do this we first need to home town of every contestant and bachelorette.  On the contestant side, there’s luckily a [wikipedia](https://en.wikipedia.org/wiki/The_Bachelorette) page that has links to every season.  We can grab all those links and add it into a list for us to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only grab bachelorette data.  This is our new table that we will be adding all the features to\n",
    "bachelorette_predict = pd.DataFrame(data_table[data_table['SHOW'] == 'Bachelorette']) \n",
    "\n",
    "\n",
    "Bachelorette_seasons = ['https://en.wikipedia.org/wiki/The_Bachelorette_(season_1)', 'https://en.wikipedia.org/wiki/The_Bachelorette_(season_2)',\n",
    "                    'https://en.wikipedia.org/wiki/The_Bachelorette_(season_3)', 'https://en.wikipedia.org/wiki/The_Bachelorette_(season_4)',\n",
    "                    'https://en.wikipedia.org/wiki/The_Bachelorette_(season_5)', 'https://en.wikipedia.org/wiki/The_Bachelorette_(season_6)',\n",
    "                    'https://en.wikipedia.org/wiki/The_Bachelorette_(season_7)', 'https://en.wikipedia.org/wiki/The_Bachelorette_(season_8)',\n",
    "                    'https://en.wikipedia.org/wiki/The_Bachelorette_(season_9)', 'https://en.wikipedia.org/wiki/The_Bachelorette_(season_10)',\n",
    "                    'https://en.wikipedia.org/wiki/The_Bachelorette_(season_11)', 'https://en.wikipedia.org/wiki/The_Bachelorette_(season_12)',\n",
    "                    'https://en.wikipedia.org/wiki/The_Bachelorette_(season_13)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these pages has a table with the contestant, age, occupation, hometown among other things.  Scraping this data turned out to be a wild ride because between finishing the project and writing it up, those wiki pages changed and instead of using pd.read_html, I needed to use beautifulsoup, etc. \n",
    "\n",
    "I'll quickly show how we can scrape the page and get the information into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                   Name   Age               Hometown           Occupation    \\\n",
      "1    Bryan Abasolo[5][6]    37         Miami, Florida         Chiropractor     \n",
      "2        Peter Kraus [8]    31     Madison, Wisconsin       Business Owner     \n",
      "3         Eric Bigger[9]    29    Baltimore, Maryland     Personal Trainer     \n",
      "4    Dean Unglert[10][6]    26        Aspen, Colorado    Startup Recruiter     \n",
      "5    Adam Gottschalk[12]    27          Dallas, Texas    Real Estate Agent     \n",
      "\n",
      "0    Outcome   Place     Ref        \n",
      "1     Winner       1     [7]        \n",
      "2  Runner-Up       2    None  None  \n",
      "3     Week 9       3    None  None  \n",
      "4     Week 8       4    [11]        \n",
      "5     Week 7       5    [13]        \n"
     ]
    }
   ],
   "source": [
    "URL= 'https://en.wikipedia.org/wiki/The_Bachelorette_(season_13)'\n",
    "response = requests.get(URL)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "My_table = soup.find(\"table\",{\"class\" :\"wikitable sortable\"})\n",
    "\n",
    "#Make a list to hold all the data we scrape from the HTML table\n",
    "contest = []\n",
    "\n",
    "#Search through the table class to find all the rows denoted by 'tr' in the HTML\n",
    "for record in My_table.findAll('tr'):\n",
    "        contest.append(record.text)\n",
    "\n",
    "#Convert list into dataframe\n",
    "contest_df = pd.DataFrame(contest)\n",
    "\n",
    "#Turns out the column data is seperated by '\\n's so we can use that to split it into a dataframe\n",
    "contest_df = contest_df.iloc[:,0].str.split('\\n', expand = True)\n",
    "\n",
    "new_header = contest_df.iloc[0] #grab the first row for the header\n",
    "contest_df.columns = new_header\n",
    "contest_df = contest_df.iloc[1:]\n",
    "print(contest_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, now the goal is to link the names to the names from the 538 table.  For example \"Bryan Abasolo[5][6]\" needs to be \"13_BRYAN_A\" and thus the hometown and age can be added to our overall table.  I choose this convention of \"Season_Firstname_LastNameLetter\" as it gives us unique names for each contestant and we don't have to worry about duplicate names.  \n",
    "\n",
    "Below is a loop that takes in a wiki URL from the list above, finds the relevant information, sorts the data into the naming convention we want, and then saves it into an overall dataframe. For a more intricate explanation of the idea feel free to read the code comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_wiki = Bachelorette_seasons\n",
    "\n",
    "wiki_df = pd.DataFrame() # Dataframe to keep everything recorded\n",
    "missed_season_tracker = []\n",
    "\n",
    "for season in seasons_wiki:\n",
    "    '''\n",
    "        This \"try: except\" method helped me figure out which seasons were giving me problems so I could go \n",
    "        back and adjust for those cases. \n",
    "    '''\n",
    "    #Ignore depreciation warnings \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    try:\n",
    "        URL= season\n",
    "        response = requests.get(URL)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        #Need to add additional try statment because the table name in the HTML for seasons 2 and 3 were different\n",
    "        try:\n",
    "            My_table = soup.find(\"table\",{\"class\" :\"wikitable sortable\"})\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            My_table = soup.find(\"table\",{\"class\" :\"wikitable\"})\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #Make a list to hold all the data we scrape from the HTML table\n",
    "        contest = []\n",
    "        \n",
    "        #Search through the table class to find all the rows denoted by 'tr' in the HTML\n",
    "        for record in My_table.findAll('tr'):\n",
    "                contest.append(record.text)\n",
    "        \n",
    "        #Convert list into dataframe\n",
    "        contest_df = pd.DataFrame(contest)\n",
    "        \n",
    "        #Turns out the column data is seperated by '\\n's so we can use that to split it into a dataframe\n",
    "        contest_df = contest_df.iloc[:,0].str.split('\\n', expand = True)\n",
    "        \n",
    "        new_header = contest_df.iloc[0] #grab the first row for the header\n",
    "        contest_df.columns = new_header\n",
    "        contest_df = contest_df.iloc[1:]\n",
    "        \n",
    "        #Only grab what we need from the wiki table \n",
    "        occup = contest_df[['Name', 'Hometown','Age']]\n",
    "        \n",
    "        #Need to get which season we are working with in order to construct the name to merge the tables with\n",
    "        #Instead of inputting a list use a regrex equations to pull the season number out of the wiki url\n",
    "        season_number = int(re.findall('\\d+', URL )[0])\n",
    "        \n",
    "        occup['SEASON'] = season_number\n",
    "        \n",
    "        #Get each name and strip any links, periods, etc\n",
    "        occup['Name'] = occup['Name'].str.replace('\\d+', '')\n",
    "        occup.Name = occup.Name.str.strip('[]')\n",
    "        occup.Name = occup.Name.str.strip('.')\n",
    "        \n",
    "        #Get the age from the age column\n",
    "        occup.Age = occup.Age.str.extract('(\\d+)')\n",
    "        \n",
    "        #Since there are a varitiy of different length names, we need to make sure we are always grabbing the\n",
    "        #right first name when using indexing.  We also have to account for cases where a \"nickname\" is recorded.\n",
    "        \n",
    "        #If they have the nickname grab the middle one\n",
    "        occup.loc[occup['Name'].str.split().str.len() == 3, 'First_name'] = occup['Name'].str.split().str[1]\n",
    "        #If they have just two names grab the first\n",
    "        occup.loc[occup['Name'].str.split().str.len() == 2, 'First_name'] = occup['Name'].str.split().str[0]\n",
    "        #If they have just one name like in the ealier seasons\n",
    "        occup.loc[occup['Name'].str.split().str.len() == 1, 'First_name'] = occup['Name'].str.split().str[0]\n",
    "        \n",
    "        #strip the parathesis\n",
    "        occup.First_name = occup.First_name.str.strip(' \"\" ')\n",
    "        \n",
    "        '''\n",
    "            Same problem as above.  Varity of ways names were recorded.  Our goal here is just to get the first letter\n",
    "            but in the first few seasons just first names were recorded.  We can infill an 'X' here so it matches our running\n",
    "            table. \n",
    "        '''\n",
    "        occup.loc[occup['Name'].str.split().str.len() == 3, 'Last_name'] = occup.Name.str.split().str[-1]\n",
    "        occup.loc[occup['Name'].str.split().str.len() == 2, 'Last_name'] = occup.Name.str.split().str[-1]\n",
    "        occup.loc[occup['Name'].str.split().str.len() == 1, 'Last_name'] = 'X'\n",
    "        \n",
    "        occup['Last_name'] = occup['Last_name'].astype(str).str[0]\n",
    "                \n",
    "        #Link everything together\n",
    "        occup[\"Name\"] = occup[\"First_name\"].map(str) + '_' + occup[\"Last_name\"]\n",
    "                \n",
    "        '''\n",
    "            The first 9 seasons in our main table have a '0#_Name' convention compared to a '#_Name' we get here.\n",
    "            Need to add that small change into here.\n",
    "        '''\n",
    "        if occup.SEASON.iloc[0] > 9:\n",
    "            occup[\"Name\"] = occup[\"SEASON\"].map(str) + '_' + occup[\"Name\"]           \n",
    "        else:\n",
    "            occup[\"Name\"] = '0'+ occup[\"SEASON\"].map(str) + '_' + occup[\"Name\"]\n",
    "        \n",
    "        #strip any hidden spaces\n",
    "        occup.Name = occup.Name.str.strip()\n",
    "        occup.Name = occup.Name.str.upper()\n",
    "        \n",
    "        #Rename it to match the elim_data table\n",
    "        occup.rename(columns={'Name':'CONTESTANT'}, inplace=True)\n",
    "        \n",
    "        #Add it to a running table\n",
    "        wiki_df = pd.concat([wiki_df, occup], sort = True)\n",
    "        \n",
    "        '''    \n",
    "            If anything above throws an error we can record which season was missed and print it out.  \n",
    "        '''\n",
    "    except:\n",
    "        print('Missed season: ' + season)\n",
    "        missed_season_tracker.append(season)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Age    CONTESTANT First_name                   Hometown Last_name  SEASON\n",
      "1  28     01_RYAN_S       Ryan             Vail, Colorado         S       1\n",
      "2  28  01_CHARLIE_M    Charlie  Hermosa Beach, California         M       1\n",
      "3  31  01_RUSSELL_W    Russell     San Rafael, California         W       1\n",
      "4  28     01_GREG_T       Greg        Manhattan, New York         T       1\n",
      "5  31      01_BOB_G        Bob         Ferndale, Michigan         G       1\n"
     ]
    }
   ],
   "source": [
    "print(wiki_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool.  Now that we have the Wikipedia data in a standard format, we can link it to our original \"elim_data\" and form \"running_table.\"  This table will be a staging area of sorts that we can use to further build our goal data set of \"Bachelorette_predict.\"  This way we can mess around with the data and not worry about changing the original elim_data.\n",
    "\n",
    "Now let's merge the two tables together on the contestant names. Once we do that let's do a quick check to make sure the tables are the same length, and nothing got lost in the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running_table length is 304\n",
      "wiki_df length is 338\n"
     ]
    }
   ],
   "source": [
    "elim_data.CONTESTANT = elim_data.CONTESTANT.str.strip()\n",
    "\n",
    "running_table = pd.merge(elim_data, wiki_df[['CONTESTANT','Hometown', 'Age']], on = 'CONTESTANT')\n",
    "\n",
    "#Cast the season number and age number into a float so that we can compare \n",
    "running_table.SEASON = running_table.SEASON.astype(float)\n",
    "running_table.Age = running_table.Age.astype(int)\n",
    "\n",
    "print(\"running_table length is \" + str(len(running_table)))\n",
    "print(\"wiki_df length is \" + str(len(wiki_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dang, it seems like we lost ~30 contestants or 10% of our data.  We can either just carry on and drop the data, but I have a feeling that we can do a bit better.  We can make a list of all the contestants that aren't in the data set and see what's up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'08_RANDY_W', '03_RYAN_S', '03_A.W.', '03_BEN_S', '03_ANDREW_X', '03_JASON_X', '03_RYAN_SH', '07_MICKEY_M', '09_MIKE_R', '03_STU_X', '09_JUAN_G', '04_FREDERICK_G', '01_RUSS_X', '11_CORY_S', '08_LUYENDYK,_J', '10_NAN_N', '02_LANNY_X', '04_GREG_M', '03_LE_P', '03_COLLIN_E', '04_BRIAN_T', '03_KEVIN', '07_NAN_N', '05_BRYAN_V', '05_BRIAN_V', '03_KEITH_X', '03_WENDELL_X', '03_MARK', '11_STANSELL_(', '11_COREY_S', '08_ARIE_L', '11_SHIVAR_(', '07_JP_R', '03_JASON_I', '08_JEANPAUL_L', '08_LEE_W', '03_KEVIN_X', '11_JJ_L', '03_STU_L', '03_DAVID_V', '06_ROBERTO_M', '04_JONATHAN_K', '12_VINCENT_V', '03_A.W_X', '03_JOHN PAUL_M', '03_MARK_X', '03_COLLIN_X', '09_MIKE_X', '02_LANNY_L', '03_KEITH', '03_DAVID_X', '12_VINNY_V', '04_GREGORY_M', '10_JJ_O', '03_BEN_X', '03_PAUL_M', '04_JON_K', '04_CHRISTOPHER_B', '03_FABRICE_L', '11_NAN_N', '06_ROBERT_M', '04_CHRIS_B', '03_WENDELL_J', '01_RUSSELL_W', '08_JEAN-PAUL_L', '04_FRED_G', '09_PABLO_G', '03_RYAN_SM', '03_ANDREW'}\n"
     ]
    }
   ],
   "source": [
    "missing = set(wiki_df.CONTESTANT) ^ set(bachelorette_predict.CONTESTANT)\n",
    "\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking closely, it seems like some of them are only 1 or 2 letters off (i.e '03_KEVIN' vs '03_KEVIN_X').  We can write a quick function that will compare two strings and return true if they're only two letters different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEditDistanceTwo(s1, s2): \n",
    "  \n",
    "    # Find lengths of given strings \n",
    "    m = len(s1) \n",
    "    n = len(s2) \n",
    "    # If difference between lengths is more than 2, \n",
    "    # then strings can't be at one distance \n",
    "    if abs(m - n) > 2: \n",
    "        return False \n",
    "    count = 0    # Count of isEditDistanceTwo \n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < m and j < n: \n",
    "        # If current characters dont match \n",
    "        if s1[i] != s2[j]: \n",
    "            if count == 2: \n",
    "                return False \n",
    "            # If length of one string is\n",
    "            # more, then only possible edit \n",
    "            # is to remove a character \n",
    "            if m > n: \n",
    "                i+=1\n",
    "            elif m < n: \n",
    "                j+=1\n",
    "            else:    # If lengths of both strings is same \n",
    "                i+=1\n",
    "                j+=1\n",
    "            # Increment count of edits \n",
    "            count+=1\n",
    "        else:    # if current characters match \n",
    "            i+=1\n",
    "            j+=1\n",
    "    # if last character is extra in any string \n",
    "    if i < m or j < n: \n",
    "        count+=1\n",
    "    return count == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then iterate through the two different tables and change the \"wiki_df\" contestant name to the actual name if it's only two letters different from a contestant entry on the table we're trying to merge on.  For example \"03_KEVIN_X\" on the wiki_df table would return true when compared to \"03_KEVIN\" on bachelorette_predict table  The wiki_df entry would then be changed to \"03_KEVIN\" and we can then easily match the Wikipedia data to that contestant.  For further details on how this is done, feel free to read the code comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the names that are in the wiki_df but not our target dataframe\n",
    "wiki_missing = list(missing & set(wiki_df.CONTESTANT))\n",
    "\n",
    "#Get the names that are in our target dataframe but not our wiki_df\n",
    "predict_missing = list(missing & set(bachelorette_predict.CONTESTANT))\n",
    "\n",
    "wiki_name = []\n",
    "predict_name = []    \n",
    "\n",
    "#Check if the missing names from the wiki_df are only one or two letters away from the target dataframe names\n",
    "for wiki_element in wiki_missing:\n",
    "    for predict_element in predict_missing:\n",
    "        if isEditDistanceTwo(wiki_element,predict_element):\n",
    "                #record the slightly different names in to two lists. \n",
    "                wiki_name.append(wiki_element)\n",
    "                predict_name.append(predict_element)\n",
    "                \n",
    "# Convert to dictionary to avoid another double loop\n",
    "convert_dict = dict(zip(wiki_name, predict_name))\n",
    "\n",
    "#Find where the names in wiki_df are only one or two leters off and replace them with the standard name format version.\n",
    "for counter in range(0, len(wiki_df)):\n",
    "    contest = wiki_df.iloc[counter,1]\n",
    "    if contest in convert_dict.keys():\n",
    "        wiki_df.iloc[counter,1] = convert_dict.get(contest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try merging again and print out the table lengths to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running_table length is 321\n",
      "wiki_df length is 338\n"
     ]
    }
   ],
   "source": [
    "elim_data.CONTESTANT = elim_data.CONTESTANT.str.strip()\n",
    "\n",
    "running_table = pd.merge(elim_data, wiki_df[['CONTESTANT','Hometown', 'Age']], on = 'CONTESTANT')\n",
    "\n",
    "#Cast the season number and age number into a float so that we can compare \n",
    "running_table.SEASON = running_table.SEASON.astype(float)\n",
    "running_table.Age = running_table.Age.astype(int)\n",
    "\n",
    "print(\"running_table length is \" + str(len(running_table)))\n",
    "print(\"wiki_df length is \" + str(len(wiki_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, now we're only missing about 17 names which is about 5% of our data. \n",
    "\n",
    "\n",
    "## Find the Cultural Region of Each Contestant\n",
    "\n",
    "Remember the overarching goal was to use this hometown data to see if the contestants are from the same cultural region and even the same hometown as the bachelorette.  This leads into the question of how do we split up the \"cultural\" regions of the United States?  Digging around I found [this article](https://www.businessinsider.com/regional-differences-united-states-2018-1) where journalist Colin Woodard broke the US down into 11 different cultural regions.  While this map would have been great to use, it was difficult to quickly find a list of the counties in each of these regions.  Instead I found a [britannica](https://www.britannica.com/place/United-States/The-newer-culture-areas) article that gave a nice breakdown.  We can then make some lists of which states below in each region.  I know there are a bunch of ways to slice this and some states are in multiple regions, however, this breakdown is a good first attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_england = ['Maine', 'Vermont', 'New Hampshire', 'Massachusetts', 'Rhode Island', 'Connecticut'] \n",
    "south = ['Alabama','Florida', 'Georgia', 'Kentucky', 'Louisiana', 'Mississippi'\n",
    "         'North Carolina', 'Oklahoma', 'Virginia', 'West Virgina', \n",
    "         'Virgina', 'Maryland']\n",
    "midatlatic = ['Pennsylvania', 'New Jersey', 'Delaware', 'New York']\n",
    "upper_midwest = ['Ohio','Indiana', 'Illinois', 'Michigan','Wisconsin', 'Iowa', 'Minnesota','Nebraska',\n",
    "                 'North Dakota', 'South Dakota', 'Nebraska']\n",
    "lower_midwest = ['Kansas', 'Missouri']\n",
    "northern_mountain = ['Montana', 'Idaho', 'Wyoming']\n",
    "northwest = ['Washington', 'Oregon']\n",
    "southwest = ['Arizona', 'New Mexico', 'Texas', 'Oklahoma', 'Arizona']\n",
    "mountain = ['Colorado','Utah']\n",
    "west = ['California', 'Nevada', 'Alaska', 'Hawaii']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write a function that will return the name of the region the hometown of the contestants are from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findregion(ind):\n",
    "    homestate = ind.get(key = 'Home State')\n",
    "    if homestate in new_england:\n",
    "        return 'New England'\n",
    "    elif homestate in south:\n",
    "        return 'South'\n",
    "    elif homestate in midatlatic:\n",
    "        return 'Midatlatic'    \n",
    "    elif homestate in upper_midwest:\n",
    "        return 'Upper midwest'\n",
    "    elif homestate in lower_midwest:\n",
    "        return 'Lower Midwest'\n",
    "    elif homestate in northern_mountain:\n",
    "        return 'Northern Mountain'    \n",
    "    elif homestate in northwest:\n",
    "        return 'Northwest'    \n",
    "    elif homestate in southwest:\n",
    "        return 'Southwest'    \n",
    "    elif homestate in mountain:\n",
    "        return 'Mountain'    \n",
    "    elif homestate in west:\n",
    "        return 'West'   \n",
    "    #In case the contestant comes from outside the US\n",
    "    else:\n",
    "        return homestate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then grab the home state from the hometown column and then run it through that function.  Finally, we'll quickly check if there's any null values denoting we missed a contestant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing contestants: 0\n"
     ]
    }
   ],
   "source": [
    "running_table['Home State'] = running_table['Hometown'].str.split(\",\").str[1].str.strip()\n",
    "    \n",
    "running_table['Home State'] = running_table['Home State'].str.strip()\n",
    "running_table['Culture Region'] = running_table.apply(findregion, axis = 1)\n",
    "\n",
    "#Check if there is any missing \n",
    "print('Number of missing contestants: ' +str(running_table['Culture Region'].isnull().any().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice.  Now we need to get the bachelorette data to match. I went through the Wikipedia pages and put the hometown, season, and age of each bachelorette in an excel sheet.  I am sure given more time, there's a better computational way to do this but with only 13 entries sometimes you just gotta do it the hard way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Bachelorette  Season                    Hometown  Age      Home State  \\\n",
      "0    Rachel Lindsay      13               Dallas, Texas   32           Texas   \n",
      "1     JoJo Fletcher      12               Dallas, Texas   26           Texas   \n",
      "2     Trista Sutter       1       Indianapolis, Indiana   31         Indiana   \n",
      "3  Meredith Philips       2           Beaverton, Oregon   30          Oregon   \n",
      "4  Jennifer Schefft       3                Mentor, Ohio   29            Ohio   \n",
      "5     DeAnna Pappas       4           Marietta, Georgia   27         Georgia   \n",
      "6    Jillian Harris       5        Peace River, Alberta   30         Alberta   \n",
      "7    Ali Fedotowsky       6  North Adams, Massachusetts   26   Massachusetts   \n",
      "8     Ashley Hebert       7            Madawaska, Maine   26           Maine   \n",
      "9     Emily Maynard       8   Charlotte, North Carolina   26  North Carolina   \n",
      "\n",
      "   Culture Region  \n",
      "0       Southwest  \n",
      "1       Southwest  \n",
      "2   Upper midwest  \n",
      "3       Northwest  \n",
      "4   Upper midwest  \n",
      "5           South  \n",
      "6         Alberta  \n",
      "7     New England  \n",
      "8     New England  \n",
      "9  North Carolina  \n"
     ]
    }
   ],
   "source": [
    "bachelorettesHT = pd.read_excel('Bachelorette_Data/Hometown_Bacherlorette.xlsx')\n",
    "\n",
    "bachelorettesHT['Home State'] = bachelorettesHT['Hometown'].str.split(\",\").str[1].str.strip()\n",
    "bachelorettesHT['Culture Region'] = bachelorettesHT.apply(findregion, axis = 1)\n",
    "print(bachelorettesHT.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now go through and see if the \"culture region\" and \"Hometown\" columns match between the contestants and their respective bachelorettes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       CONTESTANT  Match City  Match Region\n",
      "0      13_BRYAN_A           0             0\n",
      "1      13_PETER_K           0             0\n",
      "2       13_ERIC_B           0             0\n",
      "3       13_DEAN_U           0             0\n",
      "4       13_ADAM_G           1             1\n",
      "5       13_MATT_M           0             0\n",
      "6       13_ALEX_B           0             0\n",
      "7       13_WILL_G           0             0\n",
      "8      13_KENNY_L           0             0\n",
      "9    13_ANTHONY_B           0             0\n",
      "10    13_JOSIAH_G           0             0\n",
      "11       13_LEE_G           0             0\n",
      "12      13_IGGY_R           0             0\n",
      "13  13_JONATHAN_T           0             0\n",
      "14      13_JACK_S           1             1\n",
      "15     13_BRADY_E           0             0\n",
      "16     13_BRYCE_P           0             0\n",
      "17     13_DIGGY_M           0             0\n",
      "18      13_FRED_J           1             1\n",
      "19     13_BLAKE_E           0             0\n"
     ]
    }
   ],
   "source": [
    "running_table['Match Region'] = 0\n",
    "running_table['Match City'] = 0\n",
    "\n",
    "for row in range(0,len(bachelorettesHT)):\n",
    "    for contest in range(0, len(running_table)):\n",
    "        #tests if the seasons match and the regions match\n",
    "        #Match regions\n",
    "        if (bachelorettesHT.iloc[row,1] == running_table.iloc[contest, 1]) and (bachelorettesHT.iloc[row, -1] == running_table.iloc[contest, 27]):\n",
    "            running_table.iloc[contest,-2] = 1\n",
    "            #Match City\n",
    "        if (bachelorettesHT.iloc[row,1] == running_table.iloc[contest, 1]) and (bachelorettesHT.iloc[row, 2] == running_table.iloc[contest, 24]):\n",
    "            running_table.iloc[contest,-1] = 1\n",
    "            \n",
    "#Add this data to the prediction table.\n",
    "bachelorette_predict = pd.merge(bachelorette_predict, running_table[['CONTESTANT','Match Region', 'Match City']], on = 'CONTESTANT')      \n",
    "  \n",
    "#Print out to see what we got\n",
    "print(running_table[['CONTESTANT','Match City', 'Match Region']].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some of the contestant's are even from the same hometown!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Political Leanings of each Contestant\n",
    "\n",
    "Next, let's get the political leanings of each contestant.  Like with the previous features there's several ways to do this.  At first glance, we could assign each state conservative or liberal based on who they voted for in 2016.  After digging around, however, I found this concept called the [Cook Partisan Voting Index](https://en.wikipedia.org/wiki/Cook_Partisan_Voting_Index) (PVI) where each state is assigned a political leaning and a number where that number is the difference between the country's average and the state average.  For example, Alabama is Republican +15 which means if the country voted say 40% republican Alabama voted 55% republican.  While we could try to go down to the district level, let's use the state’s first. \n",
    "\n",
    "On a different note, while writing this up the wiki article changed multiple times.  So, for the sake of reproducibility, I will write the scrape code into comments and load the data from a saved csv file.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        State   PVI\n",
      "1     Alabama  R+14\n",
      "2      Alaska   R+9\n",
      "3     Arizona   R+5\n",
      "4    Arkansas  R+15\n",
      "5  California  D+12\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "voting_df = pd.read_html('https://en.wikipedia.org/wiki/Cook_Partisan_Voting_Index') \n",
    "\n",
    "state_leanings = voting_df[6]\n",
    "state_leanings.columns = state_leanings.iloc[0]\n",
    "state_leanings = state_leanings.drop(state_leanings.index[0])\n",
    "state_leanings = state_leanings.drop(['Party ofGovernor', 'Partyin Senate', 'Housebalance'], axis = 1)\n",
    "\n",
    "state_leanings.to_csv('state_leanings.csv')\n",
    "'''\n",
    "state_leanings = pd.read_csv('Bachelorette_Data/state_leanings.csv', index_col = 0)\n",
    "print(state_leanings.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before matching these numbers to the contestants, I did notice some of the contestants (and even a bachelorette!) are from Canada.  So, let's first grab the PVI for the different Canadian territories so we don't miss out on that data. We can go to Canada's Wikipedia page that has a table of the territory and each respective political leanings.  Like the PVI wiki page, this page changed multiple times even while writing this project up.  So again, for the sake of reproducibility, I'll write the scrape code in comments and have saved csv file from that output we can just load in.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Province/Territory           Canada Leanings\n",
      "1                     Alberta              Centre-right\n",
      "2            British Columbia  Centre-left to left-wing\n",
      "3                    Manitoba              Centre-right\n",
      "4               New Brunswick    Centre to centre-right\n",
      "5   Newfoundland and Labrador     Centre to centre-left\n",
      "6                 Nova Scotia     Centre to centre-left\n",
      "7                     Ontario              Centre-right\n",
      "8        Prince Edward Island              Centre-right\n",
      "9                      Quebec              Centre-right\n",
      "10               Saskatchewan              Centre-right\n",
      "11      Northwest Territories               Nonpartisan\n",
      "12                    Nunavut               Nonpartisan\n",
      "13                      Yukon     Centre to centre-left\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    canada_pol = pd.read_html('https://en.wikipedia.org/wiki/Provinces_and_territories_of_Canada')\n",
    "\n",
    "    canada_pol = canada_pol[3]\n",
    "    canada_pol.columns = canada_pol.iloc[0]\n",
    "    canada_pol = canada_pol.drop(canada_pol.index[0])\n",
    "\n",
    "    #%% Columns seemed to be fixed\n",
    "    #canada_pol = canada_pol.iloc[:-2,[0,4]]\n",
    "    #%%\n",
    "    canada_pol.rename(columns={'Majority/Minority':'Canada Leanings'}, inplace=True)\n",
    "    canada_pol['Canada Leanings'] = canada_pol['Canada Leanings'].str.replace('\\d+', '')\n",
    "    canada_pol['Canada Leanings'] = canada_pol['Canada Leanings'].str.strip('[]')\n",
    "\n",
    "    #Consolidate and remove last two rows\n",
    "    canada_pol = canada_pol[['Province/Territory', 'Canada Leanings']].iloc[:-2]\n",
    "\n",
    "    canada_pol.to_csv('Canada_Wiki.csv')\n",
    "'''\n",
    "canada_pol = pd.read_csv('Bachelorette_Data/Canada_Wiki.csv', index_col = 0)\n",
    "print(canada_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assign PVI numbers to each territory based on its leanings.  For example, we can denote \"Centre-right\" as 5.  Having some foresight, when we get around to assigning PVI numbers to contestants, let's make democrats negative numbers and republicans positive.  This way when we can take the difference in PVI numbers between contestants and bachelorettes, couples that lean in different ideologies will have a bigger difference.  Moreover, we can make this assumption cause in the 2016 election roughly the same number of people voted conservative and liberal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Province/Territory  Canada Leanings\n",
      "1                     Alberta                2\n",
      "2            British Columbia               -7\n",
      "3                    Manitoba                2\n",
      "4               New Brunswick                5\n",
      "5   Newfoundland and Labrador               -5\n",
      "6                 Nova Scotia               -5\n",
      "7                     Ontario                2\n",
      "8        Prince Edward Island                2\n",
      "9                      Quebec                2\n",
      "10               Saskatchewan                2\n",
      "11      Northwest Territories                0\n",
      "12                    Nunavut                0\n",
      "13                      Yukon               -5\n"
     ]
    }
   ],
   "source": [
    "def setCanPolitical(ind):\n",
    "    lean = ind.get(key = 'Canada Leanings')\n",
    "    if lean == 'Centre-right':\n",
    "        return 2\n",
    "    elif lean == 'Centre-left to left-wing':\n",
    "        return -7\n",
    "    elif lean == 'Centre to centre-right':\n",
    "        return 5    \n",
    "    elif lean == 'Centre to centre-left':\n",
    "        return -5\n",
    "    #non partisan\n",
    "    else:\n",
    "        return 0\n",
    " \n",
    "canada_pol['Canada Leanings'] = canada_pol.apply(setCanPolitical, axis = 1)\n",
    "print(canada_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, now let's write a function that will intake contestant data and match their home state to a PVI number.  I also included matching the Canada PVI numbers we created.  Eventually we will want to look at the bachelor data (when that season rolls around) so writing a reusable function now will save us time down the road.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindPolLean(on_going_table, PVI_table, canada_table):\n",
    "    '''\n",
    "        Intakes data table you are working with and the polictical table pulled from\n",
    "        the internet and gives you a number that indicates their PVI - is Liberal + is conservative\n",
    "        0 is either even or not in the USA\n",
    "        On going table should have a 'Home State' column and PVI_Table should have a \"State\" table\n",
    "        \n",
    "        Canada table needs to have table labeled \"Province/Territory\"\n",
    "    '''\n",
    "    \n",
    "    #Merge an input table with the contestants with a list of PVI for each state\n",
    "    on_going_table = on_going_table.merge(PVI_table, how = 'left', left_on = 'Home State', right_on = 'State')\n",
    "    on_going_table = on_going_table.replace('EVEN', 'N+0')\n",
    "    on_going_table['PVI'] = on_going_table['PVI'].astype(str)\n",
    "\n",
    "    #Need to split the values based on political parties\n",
    "    on_going_table['PVI'] = on_going_table['PVI'].str.split(\"+\") \n",
    "    \n",
    "    #Write function to put the PVI on the +- spectrum we talked about.\n",
    "    def setPolitical(ind):\n",
    "        pair = ind.get(key = 'PVI')\n",
    "        if pair[0] == 'R':\n",
    "            return int(pair[1])\n",
    "        elif pair[0] == 'D':\n",
    "            point = int(pair[1])\n",
    "            return point*-1\n",
    "        #If they're from Canada return 0 and we'll add that later\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    #Above the setPolitical function to give the PVI number on a spectrum for each contestant\n",
    "    on_going_table['Poltical Spectrum'] = on_going_table.apply(setPolitical, axis = 1)   \n",
    "    on_going_table = on_going_table.drop(['State', 'PVI'], axis = 1)\n",
    "    \n",
    "    on_going_table = on_going_table.merge(canada_table, how = 'left', left_on = 'Home State', right_on = 'Province/Territory')\n",
    "    \n",
    "    #add Canada's leanings    \n",
    "    def addCanLean(ind):\n",
    "        canada_regions = ['Alberta','British Columbia','Manitoba','New Brunswick',\n",
    "                          'Newfoundland and Labrador','Nova Scotia','Ontario','Prince Edward Island',\n",
    "                          'Quebec','Saskatchewan','Northwest Territories','Nunavut','Yukon']\n",
    "        #Only look at Canadians \n",
    "        if ind.get(key = 'Home State') in canada_regions:\n",
    "            return ind.get(key = 'Canada Leanings')\n",
    "        else:\n",
    "            return ind.get(key = 'Poltical Spectrum')\n",
    "    \n",
    "    on_going_table['PVI'] = on_going_table.apply(addCanLean, axis = 1)\n",
    "    on_going_table = on_going_table.drop(['Poltical Spectrum', 'Canada Leanings', 'Province/Territory'], axis = 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return on_going_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply this function to our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           SHOW  SEASON  CONTESTANT    1    2    3    4    5    6   7  ...   \\\n",
      "0  Bachelorette    13.0  13_BRYAN_A   R1  NaN  NaN    R    R  NaN   R  ...    \n",
      "1  Bachelorette    13.0  13_PETER_K  NaN    R  NaN  NaN  NaN    R   R  ...    \n",
      "2  Bachelorette    13.0   13_ERIC_B  NaN  NaN    R  NaN  NaN    R   R  ...    \n",
      "3  Bachelorette    13.0   13_DEAN_U  NaN    R  NaN    R  NaN  NaN   R  ...    \n",
      "4  Bachelorette    13.0   13_ADAM_G  NaN  NaN  NaN  NaN  NaN  NaN  ED  ...    \n",
      "\n",
      "  DATES-9 DATES-10 MAX_EPISODE             Hometown Age Home State  \\\n",
      "0      D1       D1          10       Miami, Florida  37    Florida   \n",
      "1      D1       D1          10   Madison, Wisconsin  31  Wisconsin   \n",
      "2      D1      NaN          10  Baltimore, Maryland  29   Maryland   \n",
      "3     NaN      NaN          10      Aspen, Colorado  26   Colorado   \n",
      "4     NaN      NaN          10        Dallas, Texas  27      Texas   \n",
      "\n",
      "  Culture Region Match Region Match City   PVI  \n",
      "0          South            0          0   2.0  \n",
      "1  Upper midwest            0          0   0.0  \n",
      "2          South            0          0 -12.0  \n",
      "3       Mountain            0          0  -1.0  \n",
      "4      Southwest            1          1   8.0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "running_table = FindPolLean(running_table,state_leanings, canada_pol)\n",
    "print(running_table.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome.  Let's check really quick that we got the right PVI for the Canadian contestants by looking for 'Alberta' which should have a PVI number of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82     2.0\n",
       "168    2.0\n",
       "236    2.0\n",
       "Name: PVI, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_table[running_table['Home State'] == 'Alberta'].iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the same function to the bachelorette table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Bachelorette  Season               Hometown  Age Home State  \\\n",
      "0    Rachel Lindsay      13          Dallas, Texas   32      Texas   \n",
      "1     JoJo Fletcher      12          Dallas, Texas   26      Texas   \n",
      "2     Trista Sutter       1  Indianapolis, Indiana   31    Indiana   \n",
      "3  Meredith Philips       2      Beaverton, Oregon   30     Oregon   \n",
      "4  Jennifer Schefft       3           Mentor, Ohio   29       Ohio   \n",
      "\n",
      "  Culture Region  PVI  \n",
      "0      Southwest  8.0  \n",
      "1      Southwest  8.0  \n",
      "2  Upper midwest  9.0  \n",
      "3      Northwest -5.0  \n",
      "4  Upper midwest  3.0  \n"
     ]
    }
   ],
   "source": [
    "bachelorettesHT = FindPolLean(bachelorettesHT, state_leanings, canada_pol)\n",
    "print(bachelorettesHT.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Political Leanings to the Bachelorettes\n",
    "\n",
    "Let's now compare the PVI and age of each bachelorette and their respective contestants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           SHOW  SEASON  CONTESTANT    1    2    3    4    5    6   7  \\\n",
      "0  Bachelorette    13.0  13_BRYAN_A   R1  NaN  NaN    R    R  NaN   R   \n",
      "1  Bachelorette    13.0  13_PETER_K  NaN    R  NaN  NaN  NaN    R   R   \n",
      "2  Bachelorette    13.0   13_ERIC_B  NaN  NaN    R  NaN  NaN    R   R   \n",
      "3  Bachelorette    13.0   13_DEAN_U  NaN    R  NaN    R  NaN  NaN   R   \n",
      "4  Bachelorette    13.0   13_ADAM_G  NaN  NaN  NaN  NaN  NaN  NaN  ED   \n",
      "\n",
      "       ...       Age Home State Culture Region  Match Region Match City   PVI  \\\n",
      "0      ...        37    Florida          South             0          0   2.0   \n",
      "1      ...        31  Wisconsin  Upper midwest             0          0   0.0   \n",
      "2      ...        29   Maryland          South             0          0 -12.0   \n",
      "3      ...        26   Colorado       Mountain             0          0  -1.0   \n",
      "4      ...        27      Texas      Southwest             1          1   8.0   \n",
      "\n",
      "  B_PVI B_Age Political Difference Age Difference  \n",
      "0   8.0    32                 -6.0              5  \n",
      "1   8.0    32                 -8.0             -1  \n",
      "2   8.0    32                -20.0             -3  \n",
      "3   8.0    32                 -9.0             -6  \n",
      "4   8.0    32                  0.0             -5  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "bachelorette_pol_lean = pd.DataFrame({\n",
    "    \"Season\": bachelorettesHT.Season,\n",
    "    \"B_PVI\": bachelorettesHT.PVI,\n",
    "    \"B_Age\": bachelorettesHT.Age})\n",
    "\n",
    "#Now need to see how much difference between bachorlette\n",
    "#Cast the running_table into a float so that we can merge them together\n",
    "running_table = running_table.merge(bachelorette_pol_lean, left_on = 'SEASON', right_on = 'Season')\n",
    "running_table = running_table.drop('Season', axis = 1)\n",
    "\n",
    "#%% Find the difference\n",
    "running_table['Political Difference'] = running_table.PVI - running_table.B_PVI\n",
    "running_table['Age Difference'] = running_table.Age - running_table.B_Age\n",
    "\n",
    "print(running_table.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the political difference and age difference, we can merge those columns into our final dataset that we will use to try to predict eliminations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CONTESTANT          SHOW  Round_Eliminated  First_Impression_Rose  \\\n",
      "0  13_BRYAN_A  Bachelorette                 0                      1   \n",
      "1  13_PETER_K  Bachelorette                10                      0   \n",
      "2   13_ERIC_B  Bachelorette                 9                      0   \n",
      "3   13_DEAN_U  Bachelorette                 8                      0   \n",
      "4   13_ADAM_G  Bachelorette                 7                      0   \n",
      "\n",
      "   Percentage Left after D1  Match Region  Match City  Political Difference  \\\n",
      "0                      50.0             0           0                  -6.0   \n",
      "1                      80.0             0           0                  -8.0   \n",
      "2                      40.0             0           0                 -20.0   \n",
      "3                      60.0             0           0                  -9.0   \n",
      "4                       NaN             1           1                   0.0   \n",
      "\n",
      "   Age Difference  \n",
      "0               5  \n",
      "1              -1  \n",
      "2              -3  \n",
      "3              -6  \n",
      "4              -5  \n"
     ]
    }
   ],
   "source": [
    "bachelorette_predict = pd.merge(bachelorette_predict, running_table[['CONTESTANT','Political Difference', 'Age Difference']], on = 'CONTESTANT', how = 'left')\n",
    "\n",
    "#Save the data set to a csv\n",
    "bachelorette_predict.to_csv('Bachelorette_Data/Bachelorette_Predict.csv')\n",
    "\n",
    "print(bachelorette_predict.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Awesome, in [part 3], we'll look at trying to apply machine learning techniques to this constructed data set.  Obviously, there is other data we could add in such as pick order, make the PVI numbers more refined, or look at physical height differences.  I think, however, an important part of any open-ended project like this is determining when to stop.  Thus, let's try to apply some techniques to get started and let that guide us to see if we need to come back and improve.     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "7181ece0-cf42-4c63-ab7c-45dd15154ce0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
